{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDEA: The original code counts the frequencies of word, we count the frequencies of node and relationship\n",
    "# Goal: 对，把graph形式的数据（这里一行是一条边(u,r,v)）转成doc形式（一个doc是u，包含许多(r,v)words，然后喂给ETM train\n",
    "# take a look at the data\n",
    "import json\n",
    "\n",
    "json_file = \"dataset_graph_form/FB15k-237/entity2wikidata.json\"\n",
    "\n",
    "f = open(json_file)\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "def check_unique(list):\n",
    "    set_len = len(set(list))\n",
    "    list_len = len(list)\n",
    "    return set_len == list_len\n",
    "\n",
    "all_nodes = data.keys()\n",
    "check_unique(all_nodes) # data.keys() contain all nodes (unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"dataset_graph_form/FB15k-237/train.txt\",\"r\")\n",
    "tr_data = file.readlines()\n",
    "file = open(\"dataset_graph_form/FB15k-237/test.txt\",\"r\")\n",
    "ts_data = file.readlines()\n",
    "file = open(\"dataset_graph_form/FB15k-237/valid.txt\",\"r\")\n",
    "va_data = file.readlines()\n",
    "file = open(\"dataset_graph_form/FB15k-237/tryout.txt\",\"r\")\n",
    "try_data = file.readlines()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dict(raw_data):\n",
    "    def prepross_data(string):\n",
    "        string = string.strip(\"\\n\")\n",
    "        tokens = string.split(\"\\t\")\n",
    "        return tokens[0],(tokens[1],tokens[2])\n",
    "    strip_n_split = list(map(prepross_data,raw_data))\n",
    "    size = len(strip_n_split)\n",
    "    nodes = [strip_n_split[i][0] for i in range(size)]\n",
    "    unique_nodes = set(nodes)\n",
    "\n",
    "\n",
    "    result_dict = {}\n",
    "    for name in unique_nodes:\n",
    "        result_dict[name] = []\n",
    "    for row in strip_n_split:\n",
    "        if row[0] in result_dict.keys():\n",
    "            result_dict[row[0]].append(row[1])\n",
    "        else:\n",
    "            result_dict[row[0]] = []\n",
    "            result_dict[row[0]].append(row[1])\n",
    "    return result_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change from (u,r,v) -> (u, [(r1,v1), (r2,v2), (r3,v3)...]), u is the key of the dict and the value is a list of tuple containing the relationship (r,v)\n",
    "\n",
    "# or we should splite the relation into tokens?\n",
    "# /music/performance_role/regular_performances./music/group_membership/group -> [\"music\", \"performance_role\", \"regular_performances.\", ....]\n",
    "pre_ts_data = generate_dict(ts_data)\n",
    "pre_tr_data = generate_dict(tr_data)\n",
    "pre_va_data = generate_dict(va_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6f91ee79537b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '1'"
     ]
    }
   ],
   "source": [
    "x = {}\n",
    "y = x[\"1\"]\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\"e0\":0, \"e1\":1, \"e2\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, {'relation': 'sick'})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.add_node(1)\n",
    "G.add_node(2)\n",
    "G.add_edge(1,2, relation=\"affect\")\n",
    "G.add_edge(1,2, relation=\"sick\")\n",
    "list(G.edges(data=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0],\n",
       "       [0, 2, 0, 0],\n",
       "       [0, 0, 3, 0],\n",
       "       [3, 0, 0, 4]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import dia_matrix\n",
    "dia_matrix((3, 4), dtype=np.int8).toarray()\n",
    "data = np.array([[1, 2, 3, 4],[3,4,5,6]])\n",
    "offsets = np.array([0, -3])\n",
    "dia_matrix((data, offsets), shape=(4, 4)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "1 -> Sequential(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
      ")\n",
      "2 -> Linear(in_features=2, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "ln = nn.Linear(2,2)\n",
    "l = nn.Sequential(ln,ln)\n",
    "net = nn.Sequential(l, l)\n",
    "for idx, m in enumerate(net.modules()):\n",
    "    print(idx, '->', m)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
